\documentclass{article}
\usepackage{ctex}
\usepackage{amsmath}

\title{逻辑斯蒂回归与最大熵模型}
\author{W.J.Z}
\date{}

\begin{document}
	\maketitle
	\section{逻辑斯蒂回归}
	\subsection{逻辑斯蒂分布}
	设$X$为连续随机变量，$X$服从逻辑斯蒂分布是指$X$具有以下的分布函数和密度函数：
	\begin{equation}
	F\left ( x \right )=P\left ( X\leq x \right )=\frac{1}{1+e^{\frac{-\left ( x-\mu  \right )}{\gamma }}}
	\end{equation}
	\begin{equation}
	f\left ( x \right )={F}'\left ( x \right )=\frac{e^{\frac{-\left ( x-\mu  \right )}{\gamma }}}{\gamma\left ( 1+ e^{\frac{-\left ( x-\mu  \right )}{\gamma }}\right )^2 }
	\end{equation}
	$\mu $为位置参数，$\gamma > 0$为形状参数。
	\subsection{逻辑斯蒂回归模型}
	设$Y$的取值为0或者1，$w=(w^{1},w^{2},\ldots,w^{n},b)^{T}$,$x=(x^{1},x^{2},\ldots,x^{n},1)^{T}$,则二项逻辑斯蒂回归模型为如下的条件概率分布：
	\begin{equation}
	P\left ( Y=1|x \right )=\frac{exp\left ( w\cdot x \right )}{1+exp\left ( w\cdot x \right )}
	\end{equation}
	\begin{equation}
	P\left ( Y=0|x \right )=\frac{1}{1+exp\left ( w\cdot x \right )}
	\end{equation}
	一件事情发生的几率是指改事件发生的概率与该事件不发生概率的比值，对于逻辑斯蒂回归而言，改事件的对数几率为：
	\begin{equation}
	\log \frac{P\left ( Y=1|x \right ))}{1-P\left ( Y=1|x \right )}=w\cdot x
	\end{equation}
	输出$Y=1$的对数几率是输入$x$的线性函数表示的模型，线性函数的值越接近正无穷，概率值就越接近1；线性函数的值越接近负无穷，概率值就越接近0.
	
	假设离散型随机变量$Y$的取值集合为${1,2,3,...,K}$,多项逻辑斯蒂回归模型是：
	\begin{equation}
	P\left ( Y=k|x \right )=\frac{exp\left ( w_{k}\cdot x \right )}{1+\sum_{k=1}^{K-1}exp\left ( w_{k}\cdot x \right )}
	\end{equation}
	\begin{equation}
	P\left ( Y=K|x \right )=\frac{1}{1+\sum_{k=1}^{K-1}exp\left ( w_{k}\cdot x \right )}
	\end{equation}
	\subsection{模型参数估计}
	设$y_{i}\epsilon \left \{ 0,1 \right \}$应用极大似然估计法评估模型参数，设：
	\begin{equation}
	P\left ( Y=1|x \right )=\pi \left ( x \right ),P\left ( Y=0|x \right )=1-\pi \left ( x \right ),
	\end{equation}
	似然函数为：
	\begin{equation}
	 \prod_{i=1}^{N}\left [ \pi\left ( x_{i} \right ) \right ]^{y_{i}}\left [ 1-\pi\left ( x_{i} \right ) \right ]^{1-y_{i}} 
	\end{equation}
	对数似然函数为：
	\begin{align}
	L\left ( w \right ) &=\sum_{i=1}^{N}\left [ y_{i}\log \pi\left ( x_{i}\right )+\left ( 1-y_{i}\log \left ( 1-\pi\left ( x_{i} \right ) \right ) \right )  \right ]\\
	&=\sum_{i=1}^{N}\left [ y_{i}\log \frac{\pi\left ( x_{i} \right )}{1-\pi\left ( x_{i} \right )}+\log \left ( 1-\pi\left ( x_{i} \right ) \right ) \right ]\\
	&=\sum_{i=1}^{N}\left [ y_{i}\left ( w\cdot x_{i} \right )-\log \left ( 1+exp\left ( w\cdot x_{i} \right ) \right ) \right ]
	\end{align}
	使用梯度下降法求极大值。
	\section{最大熵模型}
	\subsection{最大熵原理}
	最大熵原理认为要选择的概率模型首先必须满足已有的事实，即约束条件，在没有更多信息的情况下，那些不确定的部分都是"等可能的"。
	\subsection{最大熵模型}
	设联合分布的经验分布和边缘分布的经验分布为：
	\begin{equation}
	P\left ( X=x,Y=y \right )=\frac{v\left ( X=x,Y=y \right )}{N}
	\end{equation}
	\begin{equation}
	P\left ( X=x \right )=\frac{v\left ( X=x \right )}{N}
	\end{equation}
	$v(X=x,Y=y)$表示训练数据中样本$(x,y)$出现的频数，$v(X=x)$表示训练数据中输入$x$的频数，$N$表示训练样本容量。\\
	
	\noindent 用特征函数表示输入$x$与输出$y$之间的某一个事实，其定义为：
	\begin{equation}
	f(x,y)=\begin{cases}
	1 , & \text{x与y满足某一事实}\\
	0,&\text{其他}
	\end{cases}
	\end{equation}
	
	\noindent 特征函数$f(x,y)$关于联合经验分布的期望值为：
	\begin{equation}
	E_{p\left ( x,y \right )}=\sum_{x,y}P\left ( x,y \right )f\left ( x,y \right )
	\end{equation}
	特征函数关于模型$P(Y|X)$与边缘经验分布的期望值为：
	\begin{equation}
	E_{p\left ( y|x \right )}=\sum_{x,y}P\left ( x \right )P\left ( y|x \right )f\left ( x,y \right )
	\end{equation}
	假设两个期望值相等，则：
	\begin{equation}
		E_{p\left ( x,y \right )}=E_{p\left ( y|x \right )}
		\label{limit}
	\end{equation}
	公式\ref{limit}将作为模型学习的约束条件。定义在条件概率分布的条件熵为：
	\begin{equation}
	H\left ( P \right )=-\sum _{x,y}P\left ( x \right )P\left ( y|x \right )\log P\left ( y|x \right )
	\label{model}
	\end{equation}
	公式\ref{model}为最大熵模型，式中的对数为自然对数。
	\subsection{最大熵模型的学习}
	最大熵模型的学习等价于约束最优化问题。
	\begin{equation*}
	\max H\left ( P \right )=-\sum _{x,y}P\left ( x \right )P\left ( y|x \right )\log P\left ( y|x \right )
	\end{equation*}
	\begin{equation*}
		E_{p\left ( x,y \right )}=E_{p\left ( y|x \right )}
	\end{equation*}
	\begin{equation*}
	\sum_{y}P\left ( y|x \right )=1
	\end{equation*}
	按照最优化的习惯，将求最大值问题改写为等价的求最小值问题。
	\begin{equation*}
	\min -H\left ( P \right )=\sum _{x,y}P\left ( x \right )P\left ( y|x \right )\log P\left ( y|x \right )
	\end{equation*}
	\begin{equation*}
	E_{p\left ( x,y \right )}-E_{p\left ( y|x \right )}=0
	\end{equation*}
	\begin{equation*}
	\sum_{y}P\left ( y|x \right )=1
	\end{equation*}
	将约束最优化的原始问题转换为无约束最优化的对偶问题，引入拉格朗日乘子，得：
	\begin{align*}
	L\left ( P,w \right )&=-H\left ( P \right )+w_{0}\left ( 1-\sum _{y}P\left ( y|x \right ) \right )+\sum_{i=1}^{n}w_{i}\left ( E_{p\left ( x,y \right )}-E_{p\left ( y|x \right )} \right )\\
	&=\sum _{x,y}P\left ( x \right )P\left ( y|x \right )\log P\left ( y|x \right )+w_{0}\left ( 1-\sum _{y}P\left ( y|x \right ) \right )+\sum_{i=1}^{n}w_{i}\left ( E_{p\left ( x,y \right )}-E_{p\left ( y|x \right )} \right )\\
	\end{align*}
	对$L(P,w)$对$P(y|x)$求偏导得：
	\begin{equation}
	P\left ( y|x \right )=\frac{exp\left ( \sum_{i=1}^{n}w_{i}f_{i}\left ( x,y \right ) \right )}{exp\left ( 1-w_{0} \right )}
	\end{equation}
	将$P(y|x)$带入$L(P,w)$对$w$求偏导，得出$w_{i}$的值进而得出$P(y|x)$的值。
	\end{document}